{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Implementation\n",
    "* Transformers: Thư viện phổ biến để làm việc với các mô hình ngôn ngữ lớn như BERT, GPT, LLaMA,...\n",
    "* LangChain & LangChain Community: Dùng để xây dựng ứng dụng chuỗi tác vụ với LLMs.\n",
    "* Sentence-Transformers: Dùng để tạo vector embedding chất lượng cao cho truy xuất thông tin và semantic search.\n",
    "* Wikipedia-API & Underthesea: Hỗ trợ truy xuất và xử lý văn bản tiếng Việt từ Wikipedia và NLP tiếng Việt.\n",
    "* LlamaIndex: Cung cấp các công cụ để tích hợp LLM với dữ liệu riêng, hỗ trợ truy xuất dựa trên vector store.\n",
    "* Pandas & python-dotenv: Quản lý dữ liệu và biến môi trường.\n",
    "* Weaviate Client: Thư viện dùng để kết nối và lưu trữ vector embedding trong cơ sở dữ liệu Weaviate.\n",
    "* Semantic Chunkers: Dùng để chia tài liệu theo ngữ nghĩa thay vì chỉ dựa vào độ dài token.\n",
    "* Deepseek SDK: Hỗ trợ tích hợp với mô hình DeepSeek.\n",
    "* LangChain OpenAI: Giao tiếp dễ dàng giữa LangChain và các API của OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -qU transformers\n",
    "!pip install -qU langchain langchain-community # Segment\n",
    "!pip install -qU sentence-transformers\n",
    "!pip install -q wikipedia-api underthesea # tokenization\n",
    "!pip install -q llama-index llama-index-vector-stores-weaviate\n",
    "!pip install -q pandas python-dotenv\n",
    "!pip install -q weaviate-client # Vector Store\n",
    "!pip install -q litellm semantic-router semantic-chunkers # Semantic chunking\n",
    "!pip install -q deepseek-sdk\n",
    "!pip install -q langchain-openai\n",
    "!pip install -q pymupdf\n",
    "!pip install python-docx\n",
    "!pip install --upgrade \"protobuf<6.30.0\" \"grpcio>=1.72.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xử Lý Dữ Liệu & Tiền Xử Lý\n",
    "- pandas, numpy, csv, json, os, uuid: Hỗ trợ thao tác dữ liệu, định danh và xử lý file.\n",
    "- underthesea: Tách từ tiếng Việt.\n",
    "- spacy: Tách câu và xử lý NLP cổ điển.\n",
    "- dotenv: Quản lý biến môi trường.\n",
    "- google.colab.drive: Kết nối Google Drive (chỉ dùng trong môi trường Colab).\n",
    "\n",
    "Truy Xuất Dữ Liệu & Trích Xuất Nội Dung\n",
    "- wikipediaapi: Lấy dữ liệu Wikipedia.\n",
    "- PyPDFLoader, TextLoader (LangChain): Đọc và xử lý file văn bản và PDF.\n",
    "\n",
    "Mô Hình & Vector Embedding\n",
    "- transformers: Dùng mô hình ngôn ngữ từ HuggingFace (như BERT, GPT, v.v).\n",
    "- sentence-transformers: Tạo vector semantic embedding & dùng CrossEncoder.\n",
    "- llama-index: Tạo hệ thống truy xuất thông tin dựa trên vector index.\n",
    "- langchain: Kết nối LLM với dữ liệu, xây dựng pipeline hỏi đáp.\n",
    "- deepseek-sdk: Dùng mô hình DeepSeek nếu cần.\n",
    "\n",
    "Vector Store & Weaviate\n",
    "- weaviate, weaviate.classes, weaviate-client: Lưu trữ vector và tìm kiếm tương tự.\n",
    "- Auth từ weaviate.classes.init: Xác thực kết nối API.\n",
    "\n",
    "Chia Văn Bản (Chunking / Splitting)\n",
    "- SentenceSplitter, TokenTextSplitter (LlamaIndex): Chia nhỏ văn bản dựa vào câu hoặc token.\n",
    "- TextNodeParser, SentenceWindowNodeParser,...: Tùy chọn parser để tạo node trong LlamaIndex.\n",
    "\n",
    "Mô Hình Ngôn Ngữ & Prompt\n",
    "- AutoTokenizer, AutoModel: Load tokenizer và mô hình.\n",
    "- ChatOpenAI, ChatPromptTemplate: Giao tiếp và thiết kế prompt trong LangChain.\n",
    "- SystemMessage, HumanMessagePromptTemplate: Định dạng lời nhắn đầu vào cho LLM.\n",
    "\n",
    "Đánh Giá & Hiệu Năng\n",
    "- tqdm, time: Theo dõi tiến trình.\n",
    "- multiprocessing, concurrent.futures: Xử lý song song để tăng tốc độ.\n",
    "- pprint: In đẹp kết quả kiểm thử.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import weaviate\n",
    "import csv\n",
    "import time\n",
    "import inspect\n",
    "import llama_index\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import weaviate.classes as wvc\n",
    "import wikipediaapi\n",
    "import uuid\n",
    "import torch\n",
    "import requests\n",
    "import transformers\n",
    "\n",
    "from pickle import STRING\n",
    "# Process data\n",
    "from weaviate.classes.init import Auth\n",
    "from google.colab import drive\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "# Chunking, Segment\n",
    "from underthesea import word_tokenize\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from llama_index.core.text_splitter import TokenTextSplitter, SentenceSplitter\n",
    "from llama_index.core import GPTListIndex, Document\n",
    "from llama_index.core.node_parser.text import *\n",
    "# evaluation\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "#from sentence_transformers.cross_encoder import losses\n",
    "from sentence_transformers import SentenceTransformer,CrossEncoder\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presequisite information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_PATH = '/kaggle/input/stock-dataset-qibot'\n",
    "    PRETRAINED_EMBEDDING_MODEL_PATH = 'bkai-foundation-models/vietnamese-bi-encoder'\n",
    "    PRETRAINED_RERANKER_MODEL_PATH  = 'Alibaba-NLP/gte-multilingual-reranker-base'\n",
    "    \n",
    "    deepseek_api_key = \"xxxx\"\n",
    "    wvc_api = 'xxxxx'\n",
    "    wvc_url = 'xxxxx'\n",
    "    \n",
    "    RERANKER_MODEL_NAME = 'Alibaba-NLP/gte-multilingual-reranker-base'\n",
    "\n",
    "    # TOKEN_TEXT_192s = \"Token_text_192\"\n",
    "    # SENTENCEs = \"Sentence\" \n",
    "    # STATISTICALs = \"Statistical\" \n",
    "    REGEXs = \"Regex_semantic\" # => Use this\n",
    "    SEMANTIC_CHUNKING = \"Semantic_chunking\" \n",
    "    CONVENTIONAL_CHUNKING = \"Conventional_chunking\"\n",
    "    \n",
    "    collections = {REGEXs} #,TOKEN_TEXT_192s,SENTENCEs,STATISTICALs # => dùng mỗi REGEXs\n",
    "\n",
    "    lora_r = 16\n",
    "    lora_alpha = 64\n",
    "    lora_dropout = 0.1 # 0.05\n",
    "    target_modules = ['qkv_proj']\n",
    "    task_type = \"SEQ_CLS\"\n",
    "    max_seq_length = 512\n",
    "    cp = '/kaggle/input/stock-dataset-qibot/checkpoint/checkpoint14588_r16_alpha32_dropout0.1_lr1e3_batch64_mnrLoss' #path pretrained model\n",
    "\n",
    "    def __init__(self):\n",
    "        from sentence_transformers import CrossEncoder,SentenceTransformer\n",
    "        from peft import LoraConfig\n",
    "        from transformers import AutoModelForSequenceClassification\n",
    "        self.reranking_model = CrossEncoder(self.RERANKER_MODEL_NAME,\n",
    "                     max_length=self.max_seq_length,\n",
    "                     trust_remote_code=True)\n",
    "        self.backbone_model = AutoModelForSequenceClassification.from_pretrained(self.RERANKER_MODEL_NAME,\n",
    "                                                                trust_remote_code=True)\n",
    "        self.lora_config = LoraConfig(\n",
    "            r=self.lora_r,\n",
    "            lora_alpha=self.lora_alpha,\n",
    "            target_modules=self.target_modules,\n",
    "            lora_dropout=self.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=self.task_type\n",
    "        )\n",
    "        self.embedding_model = SentenceTransformer('/kaggle/input/stock-dataset-qibot/model/biencoder_bkai_epoch25_lr64_2e5')\n",
    "    \n",
    "# config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection to Database\n",
    "* reset_schema(client): Hàm này sẽ xoá toàn bộ schema hiện có trong Weaviate, bao gồm tất cả các collections đã tồn tại. Dùng khi cần khởi tạo lại hệ thống từ đầu.\n",
    "* create_schema(client, collections)\n",
    "\n",
    "Hàm này tạo mới các collections trong Weaviate với các cấu hình sau:\n",
    "* Vectorizer: Sử dụng OpenAI để sinh vector embedding (text2vec-openai).\n",
    "* Generative Model: Dùng Cohere để hỗ trợ các truy vấn sinh văn bản (generative search).\n",
    "\n",
    "Các thuộc tính trong mỗi collection gồm:\n",
    "* source: Nguồn gốc của văn bản.\n",
    "* chunk_id: ID duy nhất cho mỗi đoạn văn bản.\n",
    "* metadata: Mảng metadata đi kèm (dùng cho filter hoặc hiển thị).\n",
    "* content: Nội dung văn bản gốc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Client: \n",
    "    def __init__(self, config,collection):\n",
    "        self.cfg = config\n",
    "        \n",
    "        self.client = weaviate.connect_to_weaviate_cloud(\n",
    "          cluster_url=self.cfg.wvc_url,\n",
    "          auth_credentials=Auth.api_key(self.cfg.wvc_api),\n",
    "          headers={}\n",
    "          #headers={'X-OpenAI-Api-key': OPENAI_API_KEY}\n",
    "        )\n",
    "        self.collection = collection\n",
    "        \n",
    "    def get_cluster(self, collection_name):\n",
    "        return self.client.collections.get(collection_name)\n",
    "\n",
    "    def connect(self):\n",
    "        self.client.connect()\n",
    "        \n",
    "    def close(self):\n",
    "        self.client.close()\n",
    "    \n",
    "    def is_ready(self):\n",
    "        return self.client.is_ready()\n",
    "    \n",
    "    def reset_schema(self):\n",
    "        \"\"\"\n",
    "        Xóa toàn bộ schema (collections) hiện tại trong Weaviate client.\n",
    "        \"\"\"\n",
    "        self.client.connect()\n",
    "        self.client.collections.delete_all()\n",
    "        self.client.close()\n",
    "        \n",
    "    \n",
    "    def create_schema(self, collections):\n",
    "        \"\"\"\n",
    "        Tạo mới các schema (collections) trong Weaviate với cấu hình cụ thể.\n",
    "        \n",
    "        Args:\n",
    "            client: Weaviate client đã kết nối.\n",
    "            collections: Danh sách tên các collections cần tạo.\n",
    "        \n",
    "        Mỗi collection được cấu hình với:\n",
    "        - Vectorizer: OpenAI text embedding\n",
    "        - Generator: Cohere LLM\n",
    "        - Các thuộc tính: document_id, stock_id, metadata, text\n",
    "        \"\"\"\n",
    "        self.client.connect()\n",
    "        try:\n",
    "            for collection_name in collections:\n",
    "                docs = self.client.collections.create(\n",
    "                  name = collection_name,\n",
    "                  vectorizer_config=wvc.config.Configure.Vectorizer.text2vec_openai(),\n",
    "                  generative_config=wvc.config.Configure.Generative.cohere(),\n",
    "                  properties=[\n",
    "                      wvc.config.Property(name=\"source\",data_type=wvc.config.DataType.TEXT),\n",
    "                      wvc.config.Property(name=\"chunk_id\",data_type=wvc.config.DataType.TEXT),\n",
    "                      wvc.config.Property(name=\"metadata\",data_type=wvc.config.DataType.TEXT_ARRAY),\n",
    "                      wvc.config.Property(name=\"content\",data_type=wvc.config.DataType.TEXT)]\n",
    "                )\n",
    "                print(f\"Đã tạo {collection_name} thành công\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Lỗi khi tạo schema: {e}\")\n",
    "        self.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def insert_batch(objs, client, collection_name):\n",
    "  BATCH_SIZE = 256\n",
    "  cluster = client.get_cluster(collection_name)\n",
    "\n",
    "  for i in range(0, len(objs), BATCH_SIZE):\n",
    "    batch = objs[i:i + BATCH_SIZE]\n",
    "    cluster.data.insert_many(batch)\n",
    "    print(f\"✅ Batch {i + 1} to {i + len(batch)} inserted successfully!\")\n",
    "\n",
    "  print(f\"Data insertion into {collection_name} completed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer,CrossEncoder,util\n",
    "\n",
    "def gen_embedding(contents):\n",
    "    contents = contents.strip().lower()\n",
    "    return embedding_model.encode(contents).tolist()\n",
    "\n",
    "embedding_model = config.embedding_model\n",
    "reranker_model = config.reranking_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def segment(segment_text, format_ = \"text\"):\n",
    "  return word_tokenize(segment_text, format_).strip().lower()\n",
    "segment(\"Tuan Anh là Sinh vien trường đại học công nghệ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from semantic_chunkers.chunkers.regex import RegexChunker\n",
    "regex_chunker = RegexChunker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import fitz\n",
    "from pathlib import Path\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def count_tokens(text: str,\n",
    "                 model: str = \"gpt-3.5-turbo\"):\n",
    "  encoding = tiktoken.encoding_for_model(model)\n",
    "  tokens = encoding.encode(text)\n",
    "  return len(tokens)\n",
    "    \n",
    "def handle_keyword(text):\n",
    "    from underthesea import pos_tag\n",
    "    # Tokenize & gán nhãn từ loại\n",
    "    tagged_words = pos_tag(text)\n",
    "\n",
    "    # Lọc danh từ (Từ loại \"N\" là danh từ)\n",
    "    nouns = [word.lower() for word, pos in tagged_words if pos == \"N\"]\n",
    "\n",
    "    count_map = {}\n",
    "    for item in nouns:\n",
    "        count_map[item] = count_map.get(item, 0) + 1\n",
    "\n",
    "    filtered_counts = {key: value for key, value in count_map.items() if value > 1 and \" \" in key}\n",
    "\n",
    "    sum_tokens = 0\n",
    "    for item in filtered_counts:\n",
    "      sum_tokens += filtered_counts.get(item)\n",
    "\n",
    "    avg_count = sum_tokens / len(filtered_counts) if filtered_counts else 0\n",
    "\n",
    "    # Lọc những danh từ có số lần xuất hiện > trung bình\n",
    "    filtered_nouns = [word.lower() for word, count in filtered_counts.items() if count > avg_count]\n",
    "\n",
    "    return filtered_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    \"\"\"Load file content based on extension (.pdf or .txt).\"\"\"\n",
    "    source = Path(file_path).stem\n",
    "    ext = Path(file_path).suffix.lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        with fitz.open(file_path) as doc:\n",
    "            content = \"\".join(page.get_text() for page in doc)\n",
    "    elif ext == \".txt\":\n",
    "        loader = TextLoader(file_path).load()\n",
    "        content = loader[0].page_content\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {file_path}\")\n",
    "        return []\n",
    "    \n",
    "    return [{\"source\": source, \"content\": content}]\n",
    "\n",
    "def format_chunk(chunks):\n",
    "  objs = []\n",
    "  for chunk in chunks:\n",
    "    text = (\" \".join(chunk.splits))\n",
    "    # keywords = handle_keyword(text_)\n",
    "    content = {\n",
    "    'metadata': handle_keyword(text), # text_array\n",
    "    'content': segment(text)\n",
    "    }\n",
    "    objs.append(content)\n",
    "  return objs\n",
    "\n",
    "def process_data(corpus, chunker=RegexChunker()):\n",
    "  objs = [] # list\n",
    "  for obj in corpus:\n",
    "    metadata=obj.get('metadata', \"default\")\n",
    "    source=obj.get('source', \"default\")\n",
    "    if not isinstance(metadata, list):\n",
    "        metadata = [metadata]\n",
    "        \n",
    "    text = obj['content']\n",
    "    chunks = format_chunk((chunker([text]), metadata)[0][0]) # Chỉ dùng cho RegexChunker\n",
    "      \n",
    "    for chunk in chunks:\n",
    "      content = chunk['content'].strip().lower()\n",
    "      objs.append(\n",
    "          wvc.data.DataObject(\n",
    "              properties={\n",
    "                  \"source\": source,\n",
    "                  \"chunk_id\": str(uuid.uuid4()),\n",
    "                  \"metadata\": metadata,\n",
    "                  \"content\": content\n",
    "              },\n",
    "              vector=gen_embedding(content)\n",
    "          )\n",
    "    )\n",
    "  print(f\"Total chunks: {len(objs)}\")\n",
    "  return objs\n",
    "\n",
    "def load_data(path):\n",
    "    docs = []\n",
    "    \n",
    "    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n",
    "        return process_url(path)\n",
    "\n",
    "    elif os.path.isfile(path):\n",
    "        docs.extend(process_file(path))\n",
    "\n",
    "    elif os.path.isdir(path):\n",
    "        for file in os.listdir(path):\n",
    "            file_path = os.path.join(path, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                docs.extend(process_file(file_path))\n",
    "    if not docs:\n",
    "        print(f\"Không tìm thấy tài liệu nào trong đường dẫn: {path}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def normalize_bi_encoder(scores):\n",
    "    return ((np.array(scores) + 1) / 2)\n",
    "\n",
    "def normalize_cross_encoder(scores):\n",
    "    return 1 / (1 + np.exp(-np.array(scores)))\n",
    "\n",
    "def hybrid_score_fusion(bi_scores, cross_scores, alpha=0.4):\n",
    "    bi_norm = normalize_bi_encoder(bi_scores)\n",
    "    cross_norm = normalize_cross_encoder(cross_scores)\n",
    "    return alpha * bi_norm + (1 - alpha) * cross_norm\n",
    "        \n",
    "class Retrieval: \n",
    "    def __init__(self, config, cluster):\n",
    "        self.cfg = config\n",
    "        self.cluster = cluster\n",
    "        self.collection_name = config.REGEXs\n",
    "        self.top_k = config.TOP_K\n",
    "        self.hybrid_factor = config.HYBRID_FACTOR\n",
    "        self.alpha = config.ALPHA   \n",
    "        self.reranker_model = config.reranking_model\n",
    "        \n",
    "    def keyword_retrieval(self, query):\n",
    "      response = self.cluster.query.bm25(\n",
    "        query=segment(query),\n",
    "        limit=top_k,\n",
    "        return_metadata=[\"score\"]\n",
    "      )\n",
    "      if response.objects is None:\n",
    "        print(f\"Error retrieval() return None\")\n",
    "        return []\n",
    "      return response.objects\n",
    "\n",
    "    def hybrid_retrieval(self, query, top_k=self.top_k, hybrid_factor=self.hybrid_factor):\n",
    "        segment_query = segment(query)\n",
    "        response = self.cluster.query.hybrid(\n",
    "            query=segment_query,\n",
    "            vector=gen_embedding(segment_query),\n",
    "            alpha=0.5,\n",
    "            limit=top_k * hybrid_factor,\n",
    "            return_metadata=[\"score\"],\n",
    "        )\n",
    "         # Loại bỏ kết quả trùng text\n",
    "        seen = set()\n",
    "        results = []\n",
    "        for obj in response.objects:\n",
    "            text = obj.properties.get(\"content\", \"\").strip()\n",
    "            if not text or text in seen:\n",
    "                continue\n",
    "            seen.add(text)\n",
    "            results.append(obj)\n",
    "            if len(results) == top_k:\n",
    "                break\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def rerankce_retrieval(self, query, top_k=self.top_k, hybrid_factor=self.hybrid_factor):\n",
    "        # Lấy nhiều hơn top_k từ bi-encoder (hybrid retrieval)\n",
    "        retrieved_docs = self.hybrid_retrieval(query)\n",
    "        if not retrieved_docs:\n",
    "            return [], [], [], []\n",
    "    \n",
    "        bi_encoder_scores = np.array([doc.metadata.score for doc in retrieved_docs])\n",
    "        \n",
    "        segment_query = segment(query)\n",
    "        passage_pairs = [(segment_query, doc.properties['content']) for doc in retrieved_docs]\n",
    "        cross_encoder_scores = np.array(self.reranking_model.predict(passage_pairs))\n",
    "        \n",
    "        fused_scores = hybrid_score_fusion(bi_encoder_scores, cross_encoder_scores, self.alpha)\n",
    "        \n",
    "        top_indices = heapq.nlargest(top_k, range(len(fused_scores)), key=fused_scores.__getitem__)\n",
    "        \n",
    "        # Chuẩn bị kết quả\n",
    "        indices = [idx+1 for idx in top_indices]\n",
    "        docs = [retrieved_docs[idx] for idx in top_indices]\n",
    "        cross_scores = [float(cross_encoder_scores[idx]) for idx in top_indices]\n",
    "        fused_scores = [float(fused_scores[idx]) for idx in top_indices]\n",
    "    \n",
    "        return indices, cross_scores, fused_scores, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = 'sk-proj-xxxxxx'\n",
    "# os.environt['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "\n",
    "RESPONSE_PROMPT = \"\"\"\n",
    "Bạn là chuyên gia tư vấn về chứng khoán và thị trường tài chính.\n",
    "Dựa trên câu hỏi: {query} và ngữ cảnh: {context}, hãy trả lời chính xác, chi tiết, rõ ràng và tự nhiên bằng Tiếng việt.\n",
    "Nếu thiếu thông tin để đưa ra kết luận chính xác, hãy yêu cầu thêm dữ liệu thay vì suy đoán.\"\"\"\n",
    "\n",
    "def gen_response(doc):\n",
    "    try:\n",
    "      query = doc['question']\n",
    "      retrieved_chunks = doc['retrieved_chunks']\n",
    "      context = \"\\n\".join([doc.get()for doc in retrieved_chunks])\n",
    "      print(f\"Content\\n{context} \\n\")\n",
    "      documents = [Document(text=context)]\n",
    "      index = GPTListIndex.from_documents(documents)\n",
    "      query_engine = index.as_query_engine()\n",
    "      prompt = RESPONSE_PROMPT.format(query=query, context=context)\n",
    "      response = query_engine.query(prompt)\n",
    "      return response.response # str\n",
    "    except Exception as e:\n",
    "        return e\n",
    "        \n",
    "# gen_response(regex[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "client = Client(config,config.REGEXs)\n",
    "cluster = client.get_cluster(config.REGEXs)\n",
    "print(client.is_ready())\n",
    "client.reset_schema()\n",
    "client.create_schema(config.collections)\n",
    "folder_path=f\"{config.DATA_PATH}/knowledge\"\n",
    "path_data = insert_batch(process_data(load_data(folder_path), regex_chunker), client, config.REGEXs)\n",
    "retrieval = Retrieval(config,cluster)\n",
    "\n",
    "query = 'Chung khoan la gi'\n",
    "context = retrieval.rerankce_retrieval(query)\n",
    "gen_response(context)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
