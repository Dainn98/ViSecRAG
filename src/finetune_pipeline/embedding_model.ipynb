{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12551261,"sourceType":"datasetVersion","datasetId":7591505},{"sourceId":12551679,"sourceType":"datasetVersion","datasetId":7916640},{"sourceId":12571661,"sourceType":"datasetVersion","datasetId":7732242},{"sourceId":12574897,"sourceType":"datasetVersion","datasetId":7604373},{"sourceId":12575081,"sourceType":"datasetVersion","datasetId":7749272}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Library Implementation","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -qU sentence-transformers\n!pip install -q faiss-cpu\n!pip install -qU underthesea\nimport json\nimport random\nimport numpy as np\n\nfrom pprint import pprint\nfrom sentence_transformers import CrossEncoder, InputExample, util\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.model_selection import KFold\n\nimport torch, gc\nimport shutil\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:51:30.626528Z","iopub.execute_input":"2025-07-25T14:51:30.627076Z","iopub.status.idle":"2025-07-25T14:53:16.620252Z","shell.execute_reply.started":"2025-07-25T14:51:30.627047Z","shell.execute_reply":"2025-07-25T14:53:16.619651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Phiên bản của Sentence-transformers\")\n!pip show sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:16.621401Z","iopub.execute_input":"2025-07-25T14:53:16.622034Z","iopub.status.idle":"2025-07-25T14:53:18.659476Z","shell.execute_reply.started":"2025-07-25T14:53:16.622009Z","shell.execute_reply":"2025-07-25T14:53:18.658758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"Number of GPUs:\", torch.cuda.device_count())\n    print(\"Current GPU index:\", torch.cuda.current_device())\n    print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:18.660519Z","iopub.execute_input":"2025-07-25T14:53:18.660818Z","iopub.status.idle":"2025-07-25T14:53:18.667155Z","shell.execute_reply.started":"2025-07-25T14:53:18.660787Z","shell.execute_reply":"2025-07-25T14:53:18.666330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nimport traceback\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom sentence_transformers import (\n    SentenceTransformer,\n    SentenceTransformerTrainer,\n    SentenceTransformerTrainingArguments,\n    SentenceTransformerModelCardData,\n)\nfrom sentence_transformers.losses import MultipleNegativesRankingLoss\nfrom sentence_transformers.training_args import BatchSamplers\nfrom sentence_transformers.cross_encoder import losses\n\nlogging.getLogger(\"transformers\").setLevel(logging.ERROR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:18.669116Z","iopub.execute_input":"2025-07-25T14:53:18.669317Z","iopub.status.idle":"2025-07-25T14:53:19.146001Z","shell.execute_reply.started":"2025-07-25T14:53:18.669300Z","shell.execute_reply":"2025-07-25T14:53:19.145257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Loading corpus","metadata":{}},{"cell_type":"code","source":"# =============================================\n# II. XỬ LÝ DỮ LIỆU (QA Dataset)\n# =============================================\n# 2a. Load the QA dataset: dataset.json\nfrom underthesea import word_tokenize\nfrom transformers import AutoTokenizer\nlogging.info(\"Read my QA training dataset\")\nwith open('/kaggle/input/stock-dataset/corpus.json', 'r',encoding ='utf-8') as f:\n    raw_data = json.load(f)\nprint(f\"Số mẫu ban đầu: {len(raw_data)}\")\n\n# 2b. Tokenizing dataset\ndef tokenize_data(dataset):\n    for sample in dataset:\n        sample['anchor'] = (word_tokenize(dataset['question'], 'text'))\n        sample['positive'] = (word_tokenize(dataset['answer'], 'text'))\n    pprint(dataset[0])\n#raw_data = tokenize_data(raw_data)        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:19.146824Z","iopub.execute_input":"2025-07-25T14:53:19.147055Z","iopub.status.idle":"2025-07-25T14:53:20.792915Z","shell.execute_reply.started":"2025-07-25T14:53:19.147034Z","shell.execute_reply":"2025-07-25T14:53:20.792094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2d. Exclude exact samples\ndef deduplicate_by_text(data, keys=[\"anchor\", \"positive\"]):\n    print(f\"Trước khi loại trùng: {len(data)} mẫu\")    \n    seen = set()\n    unique_data = []\n    for sample in data:\n        key = tuple(sample[k].strip().lower() for k in keys)\n        if key not in seen:\n            seen.add(key)\n            unique_data.append(sample)\n    print(f\"\\nSau khi loại trùng: {len(data)} mẫu\")\n    return unique_data\n\n#unique_data = deduplicate_by_text(raw_data)\n#with open('corpus_processed.json', 'w', encoding='utf-8') as f:\n#    json.dump(unique_data, f, ensure_ascii=False, indent=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:20.793909Z","iopub.execute_input":"2025-07-25T14:53:20.794201Z","iopub.status.idle":"2025-07-25T14:53:20.799307Z","shell.execute_reply.started":"2025-07-25T14:53:20.794169Z","shell.execute_reply":"2025-07-25T14:53:20.798576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = load_dataset(\"json\", data_files='/kaggle/input/stock-dataset/dataset/finetune_sentenceTransformer/corpus_processed.json')['train'] # DatasetDict => Dataset\nprint(f\"\\n Dataset đã load: {dataset}\")\ndf = dataset.to_pandas()\nprint(\"\\nPhân phối câu hỏi theo level:\")\nprint(df['level'].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:20.800108Z","iopub.execute_input":"2025-07-25T14:53:20.800296Z","iopub.status.idle":"2025-07-25T14:53:25.851319Z","shell.execute_reply.started":"2025-07-25T14:53:20.800281Z","shell.execute_reply":"2025-07-25T14:53:25.850447Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Loading model","metadata":{}},{"cell_type":"code","source":"%%capture\n# 2. Load a model to finetune with 3. (Optional) model card data\nfrom sentence_transformers.util import cos_sim\n# model = SentenceTransformer(\n#     \"bkai-foundation-models/vietnamese-bi-encoder\",\n#     model_card_data=SentenceTransformerModelCardData(\n#         language=\"vi\",\n#         license=\"apache-2.0\",\n#         model_name=\"Vietnames-Biencoder finetuned on domain stock\",\n#     )\n# )\n\n# model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\nmodel = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\n\n# 4. Define a loss function\nloss= MultipleNegativesRankingLoss(model=model, similarity_fct=cos_sim, scale=20.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:25.852318Z","iopub.execute_input":"2025-07-25T14:53:25.852629Z","iopub.status.idle":"2025-07-25T14:53:35.381872Z","shell.execute_reply.started":"2025-07-25T14:53:25.852584Z","shell.execute_reply":"2025-07-25T14:53:35.381273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Loss function","metadata":{}},{"cell_type":"markdown","source":"## MNRLoss","metadata":{}},{"cell_type":"markdown","source":"## TripletLoss","metadata":{}},{"cell_type":"code","source":"from __future__ import annotations\n\nfrom collections.abc import Iterable\nfrom enum import Enum\nfrom typing import Any\n\nimport torch.nn.functional as F\nfrom torch import Tensor, nn\n\nfrom sentence_transformers.SentenceTransformer import SentenceTransformer\nfrom sentence_transformers.util import pairwise_cos_sim, pairwise_euclidean_sim, pairwise_manhattan_sim\n\n\nclass TripletDistanceMetric(Enum):\n    \"\"\"The metric for the triplet loss\"\"\"\n\n    COSINE = lambda x, y: 1 - pairwise_cos_sim(x, y)\n    EUCLIDEAN = lambda x, y: pairwise_euclidean_sim(x, y)\n    MANHATTAN = lambda x, y: pairwise_manhattan_sim(x, y)\n\n\nclass TripletLoss(nn.Module):\n    def __init__(\n        self, model: SentenceTransformer, distance_metric=TripletDistanceMetric.EUCLIDEAN, triplet_margin: float = 5\n    ) -> None:\n        super().__init__()\n        self.model = model\n        self.distance_metric = distance_metric\n        self.triplet_margin = triplet_margin\n\n    def forward(self, sentence_features: Iterable[dict[str, Tensor]], labels: Tensor) -> Tensor:\n        embeddings = [self.model(sentence_feature)[\"sentence_embedding\"] for sentence_feature in sentence_features]\n\n        return self.compute_loss_from_embeddings(embeddings, labels)\n\n    def compute_loss_from_embeddings(self, embeddings: list[Tensor], labels: Tensor) -> Tensor:\n        \"\"\"\n        Compute the CoSENT loss from embeddings.\n\n        Args:\n            embeddings: List of embeddings\n\n        Returns:\n            Loss value\n        \"\"\"\n        rep_anchor, rep_pos, rep_neg = embeddings\n        distance_pos = self.distance_metric(rep_anchor, rep_pos)\n        distance_neg = self.distance_metric(rep_anchor, rep_neg)\n\n        losses = F.relu(distance_pos - distance_neg + self.triplet_margin)\n        return losses.mean()\n\n    def get_config_dict(self) -> dict[str, Any]:\n        distance_metric_name = self.distance_metric.__name__\n        for name, value in vars(TripletDistanceMetric).items():\n            if value == self.distance_metric:\n                distance_metric_name = f\"TripletDistanceMetric.{name}\"\n                break\n\n        return {\"distance_metric\": distance_metric_name, \"triplet_margin\": self.triplet_margin}\n\n    @property\n    def citation(self) -> str:\n        return \"\"\"\n@misc{hermans2017defense,\n    title={In Defense of the Triplet Loss for Person Re-Identification},\n    author={Alexander Hermans and Lucas Beyer and Bastian Leibe},\n    year={2017},\n    eprint={1703.07737},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:54:13.691476Z","iopub.execute_input":"2025-07-25T14:54:13.692203Z","iopub.status.idle":"2025-07-25T14:54:13.703515Z","shell.execute_reply.started":"2025-07-25T14:54:13.692167Z","shell.execute_reply":"2025-07-25T14:54:13.702653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Building hard Training set","metadata":{}},{"cell_type":"code","source":"# =============================================\n## III. Building hard Training set\n# =============================================\n# 2a. Generate hard dataset\nimport torch\nimport gc\nfrom sentence_transformers.util import mine_hard_negatives\nfrom datasets import Dataset\nfrom typing import Literal, Union\n\ndef generate_hard_negatives_dataset(\n    dataset: Union[Dataset, list[dict]],\n    model,\n    anchor_column_name: str = \"anchor\",\n    positive_column_name: str = \"positive\",\n    num_negatives: int = 1,\n    range_min: int = 5,\n    range_max: int = 300,\n    max_score: float = 0.8,\n    margin: float = 0.05,\n    relative_margin: float = 0.1,\n    sampling_strategy: Literal[\"top\", \"random\"] = \"random\",\n    batch_size: int = 128,\n    as_triplets: bool = True,\n    use_faiss: bool = True,\n    verbose: bool = True,\n):\n    \"\"\"\n    Sinh tập dữ liệu hard negatives để huấn luyện mô hình Bi-Encoder hoặc Cross-Encoder.\n\n    Args:\n        dataset: Tập dữ liệu đầu vào, dạng Huggingface Dataset hoặc list các dicts có các trường anchor / positive.\n        model: SentenceTransformer đã được huấn luyện (hoặc đang fine-tune).\n        anchor_column_name: Tên cột chứa anchor (ví dụ: câu hỏi).\n        positive_column_name: Tên cột chứa positive (ví dụ: câu trả lời đúng).\n        num_negatives: Số negative cần sinh cho mỗi anchor.\n        range_min/range_max: Giới hạn range tìm kiếm candidates (theo chỉ số).\n        max_score: Điểm tương tự tối đa giữa anchor và candidate để được chọn làm negative.\n        margin/relative_margin: Khoảng cách giữa anchor-positive và anchor-negative.\n        sampling_strategy: \"top\" (ưu tiên khó nhất) hoặc \"random\" trong các candidates hợp lệ.\n        batch_size: Batch size khi tính embedding.\n        as_triplets: Nếu True sẽ sinh (anchor, positive, negative), nếu False thì sinh (anchor, positive, label).\n        use_faiss: Có dùng FAISS để tăng tốc retrieval hay không.\n        verbose: In ra log dọn cache và 1 sample ví dụ nếu True.\n\n    Returns:\n        List các triplets (hoặc pair + label), tùy thuộc vào as_triplets.\n    \"\"\"\n\n    # Dọn bộ nhớ trước khi bắt đầu\n    if verbose:\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    hard_dataset = mine_hard_negatives(\n        dataset=dataset,\n        model=model,\n        anchor_column_name=anchor_column_name,\n        positive_column_name=positive_column_name,\n        num_negatives=num_negatives,\n        range_min=range_min,\n        range_max=range_max,\n        max_score=max_score,\n        margin=margin,\n        relative_margin=relative_margin,\n        sampling_strategy=sampling_strategy,\n        batch_size=batch_size,\n        as_triplets=as_triplets,\n        use_faiss=use_faiss,\n    )\n\n    if verbose:\n        print(f\"Generated {len(hard_dataset)} hard training samples.\")\n        from pprint import pprint\n        pprint(hard_dataset[0])\n\n    return hard_dataset\n#hard_dataset_random = generate_hard_negatives_dataset(dataset,model,'anchor','positive',num_negatives=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:35.394007Z","iopub.execute_input":"2025-07-25T14:53:35.394438Z","iopub.status.idle":"2025-07-25T14:53:35.452161Z","shell.execute_reply.started":"2025-07-25T14:53:35.394419Z","shell.execute_reply":"2025-07-25T14:53:35.451656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3a. Split hard dataset into train, validation set\nfrom datasets import Dataset, ClassLabel, DatasetDict\nimport os\n\n\ndef prepare_train_valid_test_split(hard_dataset, original_dataset, test_size_fixed=2000, val_ratio=0.2, seed=42):\n    import pandas as pd\n\n    # Bước 1: Merge dữ liệu để lấy cột `level`\n    df_hard = hard_dataset.to_pandas()\n    df_source = original_dataset.to_pandas()[[\"anchor\", \"level\"]]\n    df_source_dedup = df_source.drop_duplicates(subset=\"anchor\", keep=\"first\")\n    df_merged = df_hard.merge(df_source_dedup, on=\"anchor\", how=\"left\")\n\n    # Bước 2: Encode `level` để stratify\n    unique_levels = sorted(set(original_dataset[\"level\"]))\n    class_label = ClassLabel(names=[str(l) for l in unique_levels])\n    df_merged[\"level\"] = df_merged[\"level\"].astype(str)\n    #class_label = ClassLabel(names=[\"1\", \"2\", \"3\"])\n    \n    \n    # Bước 3: Lấy tập test cố định 2000 mẫu theo stratified\n    num_classes = df_merged[\"level\"].nunique()  # = 3\n    n_per_class = test_size_fixed // num_classes  # 2000 // 3 = 666\n    remainder = test_size_fixed % num_classes     # 2000 % 3 = 2\n\n    # Lấy ngẫu nhiên từ mỗi lớp\n    df_test_parts = []\n    for i, level in enumerate(sorted(df_merged[\"level\"].unique())):\n        sample_n = n_per_class + (1 if i < remainder else 0)  # chia đều phần dư\n        df_test_part = df_merged[df_merged[\"level\"] == level].sample(\n            n=sample_n, random_state=seed + i\n        )\n        df_test_parts.append(df_test_part)\n    \n    # Gộp các phần thành df_test\n    df_test = pd.concat(df_test_parts).reset_index(drop=True)\n\n    # Loại bỏ các mẫu test khỏi df_merged để tạo df_remaining\n    df_remaining = df_merged.drop(df_test.index).reset_index(drop=True)\n    \n    from sklearn.model_selection import train_test_split\n    \"\"\"df_remaining, df_test = train_test_split(\n        df_merged,\n        test_size=test_size_fixed,\n        stratify=df_merged[\"level\"],\n        random_state=seed,\n    )\"\"\"\n\n    # Bước 4: Chia remaining thành train và validation theo tỷ lệ 9:1\n    df_train, df_valid = train_test_split(\n        df_remaining,\n        test_size=val_ratio,\n        stratify=df_remaining[\"level\"],\n        random_state=seed,\n    )\n\n    # Bước 5: Chuyển về Dataset + ép kiểu level\n    def convert(df):\n        df = df.reset_index(drop=True)\n        dset = Dataset.from_pandas(df)\n        dset = dset.map(lambda x: {\"level\": str(x[\"level\"])})\n        return dset.cast_column(\"level\", class_label)\n\n    train_set = convert(df_train)\n    validation_set = convert(df_valid)\n    test_set = convert(df_test)\n\n    # In thống kê\n    print(f\"Train size: {(train_set)}\")\n    print(train_set.to_pandas()[\"level\"].value_counts(normalize=True,sort=True), \"\\n\")\n    \n    print(f\"Validation size: {(validation_set)}\")\n    print(validation_set.to_pandas()[\"level\"].value_counts(normalize=True,sort=True), \"\\n\")\n    \n    print(f\"Test size: {(test_set)}\")\n    print(test_set.to_pandas()[\"level\"].value_counts(normalize=True,sort=False))\n\n    # Bước 6: Lưu dữ liệu về JSON theo định dạng chuẩn (mỗi dòng là 1 object đẹp, KHÔNG phải JSON Lines)\n    def save_to_json(dataset, path):\n        import json\n        df = dataset.to_pandas()\n        records = df.to_dict(orient=\"records\")\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(records, f, indent=2, ensure_ascii=False)\n    \n    output_dir = \"/kaggle/working/output_splits\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    save_to_json(train_set, f\"{output_dir}/train_set.json\")\n    save_to_json(validation_set, f\"{output_dir}/validation_set.json\")\n    save_to_json(test_set, f\"{output_dir}/test_set.json\")\n\n    return train_set.remove_columns(\"level\"), validation_set.remove_columns(\"level\"), test_set.remove_columns(\"level\")\n#train_set, validation_set, test_set = prepare_train_valid_test_split(hard_dataset_random, dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:35.452925Z","iopub.execute_input":"2025-07-25T14:53:35.453142Z","iopub.status.idle":"2025-07-25T14:53:35.473433Z","shell.execute_reply.started":"2025-07-25T14:53:35.453117Z","shell.execute_reply":"2025-07-25T14:53:35.472813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ntest_set = load_dataset(\"json\", data_files=\"/kaggle/input/stock-dataset/dataset/finetune_sentenceTransformer/margin0.05/test_set.json\")['train'].remove_columns(\"level\")\nprint(test_set)\n\nvalidation_set = load_dataset(\"json\", data_files=\"/kaggle/input/stock-dataset/dataset/finetune_sentenceTransformer/margin0.05/validation_set.json\")['train'].remove_columns(\"level\")\nprint(validation_set)\n\ntrain_set = load_dataset(\"json\", data_files=\"/kaggle/input/stock-dataset/dataset/finetune_sentenceTransformer/margin0.05/train_set.json\")['train'].remove_columns(\"level\")\nprint(train_set)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:35.474188Z","iopub.execute_input":"2025-07-25T14:53:35.474419Z","iopub.status.idle":"2025-07-25T14:53:37.849076Z","shell.execute_reply.started":"2025-07-25T14:53:35.474393Z","shell.execute_reply":"2025-07-25T14:53:37.848487Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. TripletEvaluator","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nfrom sentence_transformers.evaluation import TripletEvaluator\nfrom typing import List, Dict\n\ndef evaluate_with_multiple_margins(\n    model,\n    dataset: dict,\n    margins: List[float],\n    name_prefix: str = \"eval\",\n    batch_size: int = 128,\n    show_progress_bar: bool = False,\n    verbose: bool = True\n) -> Dict[float, float]:\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    results = {}\n\n    for margin in margins:\n        evaluator = TripletEvaluator(\n            anchors=dataset[\"anchor\"],\n            positives=dataset[\"positive\"],\n            negatives=dataset[\"negative\"],\n            name=f\"{name_prefix}_margin_{margin}\",\n            margin=margin,\n            batch_size=batch_size,\n            show_progress_bar=show_progress_bar\n        )\n        accuracy = evaluator(model)\n        results[margin] = accuracy\n        print(margin, accuracy)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:37.849834Z","iopub.execute_input":"2025-07-25T14:53:37.850096Z","iopub.status.idle":"2025-07-25T14:53:37.855763Z","shell.execute_reply.started":"2025-07-25T14:53:37.850068Z","shell.execute_reply":"2025-07-25T14:53:37.854851Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers.evaluation import TripletEvaluator\ndef triplet_evaluator(dataset,name:str,margin:float,batch_size=128):\n    # Dọn bộ nhớ trước khi bắt đầu\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    evaluator = TripletEvaluator(\n    anchors=dataset[\"anchor\"],\n    positives=dataset[\"positive\"],\n    negatives=dataset[\"negative\"],\n    name=name,\n    margin = margin,\n    batch_size =batch_size,\n    show_progress_bar=False\n    )\n    return evaluator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:37.856663Z","iopub.execute_input":"2025-07-25T14:53:37.857064Z","iopub.status.idle":"2025-07-25T14:53:37.913389Z","shell.execute_reply.started":"2025-07-25T14:53:37.857032Z","shell.execute_reply":"2025-07-25T14:53:37.912816Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Training Arguments","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformerTrainingArguments\nfrom sentence_transformers.trainer import BatchSamplers\n\ndef traing_argument(\n    name: str,\n    num_epochs: int,\n    batch_size: int,\n    accumulation: int,\n    learning_rate: float,\n    warmup_ratio: float = 0.1,\n    weight_decay: float = 0.01,\n    max_grad_norm: float = 1.0,\n    lr_scheduler_type: str = \"linear\",\n):\n    print(f\"Learning_rate: {learning_rate}\\nBatch_size: {batch_size*accumulation}\\nScheduler: {lr_scheduler_type}\")\n    \n    args = SentenceTransformerTrainingArguments(\n        output_dir=f\"models/{name}\",\n        # Training\n        num_train_epochs=num_epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        learning_rate=learning_rate,\n        warmup_ratio=warmup_ratio,\n        weight_decay=weight_decay,\n        max_grad_norm=max_grad_norm,\n        gradient_accumulation_steps=accumulation,\n        eval_accumulation_steps=accumulation,\n\n        fp16=True,\n        bf16=False,\n        batch_sampler=BatchSamplers.NO_DUPLICATES,\n        lr_scheduler_type=lr_scheduler_type,\n\n        # Checkpointing & Logging\n        save_strategy=\"epoch\",\n        save_total_limit=1,\n        logging_steps=10,\n        logging_dir=\"logs\",\n        logging_strategy=\"steps\",\n\n        eval_strategy=\"epoch\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n\n        report_to=[\"tensorboard\"],\n        disable_tqdm=False,\n        seed=42,\n        run_name=name\n    )\n    return args","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:37.914034Z","iopub.execute_input":"2025-07-25T14:53:37.914223Z","iopub.status.idle":"2025-07-25T14:53:37.928669Z","shell.execute_reply.started":"2025-07-25T14:53:37.914207Z","shell.execute_reply":"2025-07-25T14:53:37.928124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8. Trainer","metadata":{}},{"cell_type":"code","source":"# 7. Create a trainer & train\nimport traceback\nfrom transformers import EarlyStoppingCallback\ndef trainer(model,\n            training_args,\n            train_data,\n            eval_data,\n            loss_fn,\n            evaluator,\n            use_triplet:bool=True,\n            early_stopping_patience: int = 2\n    ):\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    print(f\"Loss: {loss_fn}\")\n\n    early_stopping = EarlyStoppingCallback(\n        early_stopping_patience=early_stopping_patience,\n        early_stopping_threshold=5e-4\n    )\n    \n    if not use_triplet:\n        print(\"Inbatch-negatives\")\n        train_dataset = train_data.remove_columns(\"negative\")\n        eval_dataset = eval_data.remove_columns(\"negative\")\n    else:\n        print(\"Triplet\")\n\n    trainer = SentenceTransformerTrainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_data,\n        eval_dataset=eval_data,\n        loss=loss_fn,\n        evaluator=evaluator,\n        callbacks=[early_stopping] \n    )\n    return trainer ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:53:37.929514Z","iopub.execute_input":"2025-07-25T14:53:37.930235Z","iopub.status.idle":"2025-07-25T14:53:37.946399Z","shell.execute_reply.started":"2025-07-25T14:53:37.930216Z","shell.execute_reply":"2025-07-25T14:53:37.945865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"dev_evaluator = triplet_evaluator(dataset=validation_set,name='validation',margin=0.1)\nprint(dev_evaluator(model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:54:50.845511Z","iopub.execute_input":"2025-07-25T14:54:50.845827Z","iopub.status.idle":"2025-07-25T14:54:51.458838Z","shell.execute_reply.started":"2025-07-25T14:54:50.845805Z","shell.execute_reply":"2025-07-25T14:54:51.457996Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"margins_to_test = [0.05, 0.1,0.15, 0.2, 0.3,0.4,0.5,0.6,0.7,0.8]\nresults = evaluate_with_multiple_margins(model=model,dataset=test_set,margins=margins_to_test,name_prefix=\"test_set\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers.util import cos_sim\n\n# model = SentenceTransformer(\"bkai-foundation-models/vietnamese-bi-encoder\",\n#     model_card_data=SentenceTransformerModelCardData(language=\"vi\",license=\"apache-2.0\",model_name=\"Vietnames-Biencoder finetuned on domain stock\",)\n# )\n# model_mpnet = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n# model_miniLM = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\nmnr_loss = MultipleNegativesRankingLoss(model=model,similarity_fct=cos_sim,scale=20.0)\ntriplet_loss = TripletLoss(model=model,distance_metric=TripletDistanceMetric.COSINE,triplet_margin=0.2)\n\nargs = traing_argument(\n    name=\"biencoder_StockVN\",\n    num_epochs=25,\n    batch_size=64,\n    accumulation=2,\n    learning_rate=2e-5,\n    warmup_ratio=0.1,\n    weight_decay=0.01,\n    max_grad_norm=1.0,\n    lr_scheduler_type=\"cosine\"\n)\ntrainer = trainer(model,\n                  args,\n                  train_set,\n                  validation_set,\n                  triplet_loss,\n                  dev_evaluator,\n                  use_triplet=True)\ntry:\n    trainer.train()\n    best_model_dir = trainer.state.best_model_checkpoint\n    if best_model_dir:\n        zip_output = best_model_dir.rstrip(os.sep) + \".zip\"\n        shutil.make_archive(base_name=best_model_dir, format='zip', root_dir=best_model_dir)\n        print(f\"✅ Đã nén model tốt nhất tại: {zip_output}\")\n    else:\n        print(\"⚠️ Không tìm thấy checkpoint tốt nhất!\")\nexcept Exception as e:\n    print(e)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T14:54:54.605370Z","iopub.execute_input":"2025-07-25T14:54:54.605702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"margins_to_test = [0.05, 0.1,0.15, 0.2, 0.3,0.4,0.5,0.6,0.7,0.8]\nresults = evaluate_with_multiple_margins(model=model,dataset=test_set,margins=margins_to_test,name_prefix=\"test_set\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}